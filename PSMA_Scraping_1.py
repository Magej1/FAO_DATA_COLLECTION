### cd PSMA_DATA
### python -m venv venv
### .\venv\Scripts\activate
# å®‰è£…seleniumï¼Œå¦‚æœæ²¡æœ‰ pip install selenium pandas
# python PSMA_scraping_1.py

### PSMA_scraping_1.py
# ç”¨äºæŠ“å– FAO PortLex å„å›½æ¸”ä¸šæ”¿ç­–ä¿¡æ¯ï¼ˆæ”¯æŒæ–­ç‚¹ç»­æŠ“ã€åŠ¨æ€å†…å®¹åŠ è½½ï¼‰

import pandas as pd
import time
import json
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup

# =====================
# åˆå§‹åŒ– Chrome é©±åŠ¨
# =====================
# def get_driver():
    # options = Options()
    # # options.add_argument("--headless")  # å¯è°ƒè¯•æ—¶å…³é—­ headless
    # options.add_argument("--disable-gpu")
    # options.add_argument("--no-sandbox")
    # driver_path = "D:/æ–‡æ¡£/DataScience/chromedriver.exe"  # ä¿®æ”¹ä¸ºä½ çš„ chromedriver è·¯å¾„
    # service = Service(driver_path)
    # return webdriver.Chrome(service=service, options=options)

def get_driver(): # æ–°ç‰ˆåŠ äº†ä»£ç†
    options = Options()
    # options.add_argument("--headless")  # è°ƒè¯•é˜¶æ®µå»ºè®®å…³é—­ headless
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--start-maximized")
    
    # æ·»åŠ ç”¨æˆ·ä»£ç†æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼ˆé‡ç‚¹ï¼ï¼‰
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/125.0.0.0 Safari/537.36"
    )

    driver_path = "D:/æ–‡æ¡£/DataScience/chromedriver.exe"
    service = Service(driver_path)
    driver = webdriver.Chrome(service=service, options=options)
    return driver

# =============================
# è§£ææ¯ä¸ªå›½å®¶é¡µé¢çš„æ•°æ®é€»è¾‘
# =============================
def parse_country_page(driver, iso3):
    url = f"https://portlex.fao.org/CountryProfile?{iso3}"
    driver.get(url)

    # âœ… ç­‰å¾…æ•´ä¸ªé¡µé¢åŠ è½½å®Œæ¯•
    WebDriverWait(driver, 15).until(
        lambda d: d.execute_script("return document.readyState") == "complete"
    )
    time.sleep(2)  # ç»™ JavaScript æ¸²æŸ“ä¸€ç‚¹æ—¶é—´

    # âœ… å†ç­‰å¾…ä½ éœ€è¦çš„æ•°æ®åŒºåŸŸåŠ è½½å‡ºæ¥ï¼ˆå¯é€‰å¢å¼ºç¨³å®šæ€§ï¼‰
    WebDriverWait(driver, 15).until(
        EC.presence_of_element_located((By.CSS_SELECTOR, ".header-parent-item span"))
    )

    # âœ… ä¹‹åå†æŠ“å–é¡µé¢ HTML
    soup = BeautifulSoup(driver.page_source, 'html.parser')

    # ä»¥ä¸‹å†…å®¹ä¸å˜ â†“â†“â†“
    country_name_tag = soup.select_one(".header-parent-item span")
    country_name = country_name_tag.get_text(strip=True) if country_name_tag else "N/A"

    intl_commitments = [a.get_text(strip=True) for a in soup.select(".treaties_div a")]
    rfmo_count = len(soup.select("ul.rfmo .collapsible-body a"))
    npoa_count = len(soup.select("ul.national_plans .collapsible-body a"))
    mrl_count = len(soup.select("ul.most_relevant .collapsible-body a"))

    psm_section = soup.select_one(".query_results_count span")
    try:
        psm_count = int(psm_section.get_text(strip=True)) if psm_section else 0
    except:
        psm_count = 0

    psm_records = []
    for record in soup.select(".result-list .parent-item-container"):
        psm_entry = {}
        rows = record.select("table tr")
        for row in rows:
            tds = row.select("td")
            if len(tds) == 2:
                key = tds[0].get_text(strip=True)
                val = tds[1].get_text(strip=True)
                psm_entry[key] = val
        psm_records.append(psm_entry)

    return {
        "ISO3": iso3,
        "Country Name": country_name,
        "International Commitments": intl_commitments,
        "Membership to RFMOs": rfmo_count,
        "National Plan Of Action": npoa_count,
        "Most Relevant Legislation": mrl_count,
        "PSM Provisions Count": psm_count,
        "PSM Provisions": psm_records
    }


# =====================
# ä¸»ç¨‹åº
# =====================
def run_scraper(iso3_csv="sample_iso3_list_TEST.csv", output_csv="fao_portlex_data_TEST.csv", output_json="fao_portlex_data_TEST.json"): ### ä¿®æ”¹ä¸ºä½ çš„ ISO3 åˆ—è¡¨æ–‡ä»¶è·¯å¾„å’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„
    df_iso3 = pd.read_csv(iso3_csv)
    processed = []

    try:
        existing_df = pd.read_csv(output_csv)
        done_set = set(existing_df["ISO3"].astype(str))
        print(f"ğŸ” Resuming from last point. Already processed {len(done_set)} countries.")
    except:
        existing_df = pd.DataFrame()
        done_set = set()

    driver = get_driver()

    for iso3 in df_iso3["ISO3"].astype(str):
        if iso3 in done_set:
            continue

        print(f"ğŸŒ Processing {iso3}...")
        try:
            result = parse_country_page(driver, iso3)
            processed.append(result)

            df_new = pd.DataFrame(processed)
            df_all = pd.concat([existing_df, df_new], ignore_index=True)
            df_all.to_csv(output_csv, index=False, encoding="utf-8-sig")

            with open(output_json, "w", encoding="utf-8") as f:
                json.dump(df_all.to_dict(orient="records"), f, ensure_ascii=False, indent=2)

            processed = []

        except Exception as e:
            print(f"âŒ Error processing {iso3}: {e}")
            continue

    driver.quit()
    print(f"\nâœ… Done. Output saved to {output_csv} and {output_json}")

# ============
# å¯åŠ¨ç¨‹åº
# ============
if __name__ == "__main__":
    run_scraper("sample_iso3_list_TEST.csv") ####### ä¿®æ”¹ä¸ºä½ çš„ ISO3 åˆ—è¡¨æ–‡ä»¶è·¯å¾„
